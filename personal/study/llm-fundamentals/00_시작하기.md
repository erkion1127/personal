# LLM ê¸°ì´ˆ ìŠ¤í„°ë”” ì‹œì‘í•˜ê¸°

## ğŸ“– í•™ìŠµ ëª©í‘œ

- Transformer ì•„í‚¤í…ì²˜ ì™„ì „ ì´í•´
- Attention ë©”ì»¤ë‹ˆì¦˜ì˜ ì›ë¦¬ íŒŒì•…
- í”„ë¡¬í”„íŠ¸ ì—”ì§€ë‹ˆì–´ë§ ë§ˆìŠ¤í„°
- RAG ì‹œìŠ¤í…œ êµ¬ì¶•
- LLM ìµœì í™” ê¸°ë²• í•™ìŠµ

## âœ… í•™ìŠµ ì²´í¬ë¦¬ìŠ¤íŠ¸

### Week 1-2: Transformer ì•„í‚¤í…ì²˜
- [ ] "Attention Is All You Need" ë…¼ë¬¸ ì •ë…
- [ ] Self-Attention ë©”ì»¤ë‹ˆì¦˜ ì´í•´
- [ ] Multi-Head Attention êµ¬ì¡°
- [ ] Positional Encoding
- [ ] Feed-Forward Networks
- [ ] Layer Normalization & Residual Connections
- [ ] ê°„ë‹¨í•œ Transformer ì§ì ‘ êµ¬í˜„ (PyTorch)

### Week 3: Tokenization & Embeddings
- [ ] Tokenization ë°©ë²• (BPE, WordPiece, SentencePiece)
- [ ] Token Embeddings vs Position Embeddings
- [ ] Vocabulary í¬ê¸°ì™€ ì„±ëŠ¥ì˜ ê´€ê³„
- [ ] Special Tokens ì´í•´
- [ ] ë‹¤ì–‘í•œ Tokenizer ë¹„êµ ì‹¤í—˜

### Week 4-5: Prompt Engineering
- [ ] Zero-shot vs Few-shot Learning
- [ ] Chain-of-Thought (CoT) Prompting
- [ ] Self-Consistency
- [ ] Tree of Thoughts (ToT)
- [ ] Prompt Injection & Security
- [ ] í”„ë¡¬í”„íŠ¸ ìµœì í™” ê¸°ë²•
- [ ] ì‹¤ì „ í”„ë¡¬í”„íŠ¸ ë¼ì´ë¸ŒëŸ¬ë¦¬ êµ¬ì¶•

### Week 6-7: Fine-tuning
- [ ] Transfer Learning ê°œë…
- [ ] Full Fine-tuning vs Parameter-Efficient Fine-tuning
- [ ] LoRA (Low-Rank Adaptation)
- [ ] QLoRA (Quantized LoRA)
- [ ] Instruction Tuning
- [ ] RLHF (Reinforcement Learning from Human Feedback)
- [ ] ì‹¤ìŠµ: ì‘ì€ ëª¨ë¸ Fine-tuning

### Week 8-9: RAG (Retrieval-Augmented Generation)
- [ ] RAG ì•„í‚¤í…ì²˜ ì´í•´
- [ ] Vector Embeddings (OpenAI, Sentence-BERT)
- [ ] Vector Databases (Chroma, Pinecone, FAISS)
- [ ] Chunking Strategies
- [ ] Hybrid Search (Vector + Keyword)
- [ ] Reranking ê¸°ë²•
- [ ] RAG ì‹œìŠ¤í…œ êµ¬ì¶• í”„ë¡œì íŠ¸

### Week 10: ìµœì í™” & ì‹¤ì „
- [ ] Context Window ê´€ë¦¬
- [ ] Token Economics (ë¹„ìš© ìµœì í™”)
- [ ] Caching ì „ëµ
- [ ] Batching & Parallel Processing
- [ ] Streaming Responses
- [ ] Error Handling & Fallbacks

## ğŸ“š í•„ìˆ˜ ìë£Œ

### í•µì‹¬ ë…¼ë¬¸
1. **Transformer**: "Attention Is All You Need" (Vaswani et al., 2017)
2. **BERT**: "BERT: Pre-training of Deep Bidirectional Transformers"
3. **GPT-3**: "Language Models are Few-Shot Learners"
4. **InstructGPT**: "Training language models to follow instructions"
5. **LoRA**: "Low-Rank Adaptation of Large Language Models"
6. **RAG**: "Retrieval-Augmented Generation for Knowledge-Intensive NLP"

### ì˜¨ë¼ì¸ ê°•ì˜
- Stanford CS224N: Natural Language Processing with Deep Learning
- DeepLearning.AI: "ChatGPT Prompt Engineering for Developers"
- Hugging Face Course: NLP with Transformers
- Fast.ai: Practical Deep Learning for Coders

### ë„ì„œ
- "Natural Language Processing with Transformers" (Hugging Face)
- "Build a Large Language Model (From Scratch)"
- "Hands-On Large Language Models"

## ğŸ”§ ê°œë°œ í™˜ê²½ ì„¤ì •

### Python íŒ¨í‚¤ì§€
```bash
# ê¸°ë³¸ ë”¥ëŸ¬ë‹
pip install torch torchvision
pip install transformers
pip install datasets

# RAG & Embeddings
pip install langchain
pip install sentence-transformers
pip install chromadb
pip install faiss-cpu

# ìœ í‹¸ë¦¬í‹°
pip install tiktoken  # OpenAI tokenizer
pip install sentencepiece
pip install numpy pandas matplotlib
```

### GPU ì„¤ì • (ì„ íƒ)
```python
import torch
print(f"CUDA available: {torch.cuda.is_available()}")
print(f"Device: {torch.cuda.get_device_name(0)}")
```

## ğŸ“ ì‹¤ìŠµ í”„ë¡œì íŠ¸

### 1. Transformer from Scratch
- PyTorchë¡œ ê°„ë‹¨í•œ Transformer êµ¬í˜„
- ë²ˆì—­ íƒœìŠ¤í¬ë¡œ í•™ìŠµ
- Attention ì‹œê°í™”

### 2. Prompt Engineering Library
- ë‹¤ì–‘í•œ íƒœìŠ¤í¬ë³„ í”„ë¡¬í”„íŠ¸ í…œí”Œë¦¿
- Few-shot ì˜ˆì œ ê´€ë¦¬
- í”„ë¡¬í”„íŠ¸ A/B í…ŒìŠ¤íŒ…

### 3. RAG ì‹œìŠ¤í…œ
- ë¬¸ì„œ ì„ë² ë”© ë° ì €ì¥
- ê²€ìƒ‰ + ìƒì„± íŒŒì´í”„ë¼ì¸
- ë‹µë³€ í’ˆì§ˆ í‰ê°€

### 4. Fine-tuning ì‹¤í—˜
- ì‘ì€ ëª¨ë¸ (GPT-2, BERT) Fine-tuning
- ì»¤ìŠ¤í…€ ë°ì´í„°ì…‹ ì¤€ë¹„
- ì„±ëŠ¥ ë¹„êµ (Base vs Fine-tuned)

## ğŸ’¡ í•™ìŠµ íŒ

1. **ìˆ˜ì‹ ì´í•´**: Attention ìˆ˜ì‹ì„ ì†ìœ¼ë¡œ ê³„ì‚°í•´ë³´ê¸°
2. **ì½”ë“œ êµ¬í˜„**: ë…¼ë¬¸ì˜ ì•Œê³ ë¦¬ì¦˜ì„ ì§ì ‘ ì½”ë“œë¡œ ì‘ì„±
3. **ì‹œê°í™”**: Attention weights, embeddings ì‹œê°í™”
4. **ì‘ì€ ë°ì´í„°**: ì²˜ìŒì—” ì‘ì€ ë°ì´í„°ì…‹ìœ¼ë¡œ ë¹ ë¥¸ ì‹¤í—˜
5. **ë¹„ìš© ê´€ë¦¬**: ë¬´ë£Œ í‹°ì–´ í™œìš© (Colab, HuggingFace)

## ğŸ¯ í•µì‹¬ ê°œë… ì •ë¦¬

### Attention ë©”ì»¤ë‹ˆì¦˜
```
Attention(Q, K, V) = softmax(QK^T / âˆšd_k)V
```
- Q (Query): ì°¾ê³ ì í•˜ëŠ” ì •ë³´
- K (Key): ë¹„êµ ëŒ€ìƒ
- V (Value): ì‹¤ì œ ê°’

### Transformer êµ¬ì¡°
```
Input â†’ Embedding â†’ [Encoder Blocks] â†’ [Decoder Blocks] â†’ Output

Encoder Block:
- Multi-Head Self-Attention
- Add & Norm
- Feed-Forward Network
- Add & Norm
```

### RAG íŒŒì´í”„ë¼ì¸
```
Query â†’ Embedding â†’ Vector Search â†’ Retrieve Docs â†’
  â†’ Combine with Prompt â†’ LLM â†’ Answer
```

## ğŸ“Š ì§„í–‰ ìƒí™©

| ì£¼ì°¨ | ì£¼ì œ | ìƒíƒœ | ì™„ë£Œì¼ |
|------|------|------|--------|
| 1-2 | Transformer ì•„í‚¤í…ì²˜ | â¬œ ëŒ€ê¸° | - |
| 3 | Tokenization | â¬œ ëŒ€ê¸° | - |
| 4-5 | Prompt Engineering | â¬œ ëŒ€ê¸° | - |
| 6-7 | Fine-tuning | â¬œ ëŒ€ê¸° | - |
| 8-9 | RAG | â¬œ ëŒ€ê¸° | - |
| 10 | ìµœì í™” | â¬œ ëŒ€ê¸° | - |

## ğŸ”— ìœ ìš©í•œ ë§í¬

### ê³µì‹ ë¬¸ì„œ
- [Hugging Face Transformers](https://huggingface.co/docs/transformers/)
- [OpenAI API Docs](https://platform.openai.com/docs/)
- [LangChain RAG](https://python.langchain.com/docs/use_cases/question_answering/)

### ì‹œê°í™” ë„êµ¬
- [Transformer Explainer](https://poloclub.github.io/transformer-explainer/)
- [BertViz](https://github.com/jessevig/bertviz)
- [Embedding Projector](https://projector.tensorflow.org/)

### ë°ì´í„°ì…‹
- [Hugging Face Datasets](https://huggingface.co/datasets)
- [Common Crawl](https://commoncrawl.org/)
- [The Pile](https://pile.eleuther.ai/)

### ë²¤ì¹˜ë§ˆí¬
- [GLUE](https://gluebenchmark.com/)
- [SuperGLUE](https://super.gluebenchmark.com/)
- [MMLU](https://github.com/hendrycks/test)

## ğŸ“ˆ ë‹¤ìŒ ë‹¨ê³„

LLM ê¸°ì´ˆë¥¼ ë§ˆì¹œ í›„:
1. **Agentic AI**ì—ì„œ LLM ê¸°ë°˜ ì—ì´ì „íŠ¸ êµ¬ì¶•
2. **Agent Frameworks**ë¡œ ì‹¤ì „ ì• í”Œë¦¬ì¼€ì´ì…˜ ê°œë°œ
3. **Knowledge Graphs**ì™€ í†µí•©í•˜ì—¬ ì§€ì‹ ê¸°ë°˜ ì‹œìŠ¤í…œ

## ğŸ“– í•™ìŠµ ì¼ì§€ í…œí”Œë¦¿

```markdown
# YYYY-MM-DD - Transformer Self-Attention

## í•™ìŠµ ëª©í‘œ
- [ ] Self-Attention ìˆ˜ì‹ ì´í•´
- [ ] ì½”ë“œë¡œ êµ¬í˜„
- [ ] ì‹œê°í™”

## í•µì‹¬ ë‚´ìš©
...

## ì½”ë“œ ìŠ¤ë‹ˆí«
```python
# Attention êµ¬í˜„
```

## ì§ˆë¬¸/ì´ìŠˆ
...

## ë‹¤ìŒ í•™ìŠµ
...
```

---

**ì‹œì‘ì¼**:
**ëª©í‘œ ì™„ë£Œì¼**:
